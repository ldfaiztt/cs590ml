\documentclass[12pt,letterpaper]{article}
\include{preamble}

% Edit these as appropriate
\newcommand\course{STA571/CS590.01}
\newcommand\semester{Spring 2014}                   % <-- current semester
\newcommand\papertitle{Gaussian Processes for Machine Learning}                         % <-- paper title
\newcommand\authoryear{Rasmussen and Williams}
\newcommand\yourname{Matt Dickenson}                % <-- your name
\newcommand\login{mcd31}                            % <-- your NetID
\newcommand\hwdate{Due: 26 February, 2014}           % <-- HW due date


\pagestyle{fancyplain}
\headheight 60pt
\chead{Summary of ``\papertitle''\\ ~\\}
\lhead{\small \yourname\ \texttt{\login}\\\course}
\rhead{\small \hwdate}
\headsep 10pt

\begin{document}

% Read and Summarize sections 2.1 - 2.3 of the book "Gaussian Processes for Machine Learning" by Rasmussen and Williams (available online).

Rasmusssen and Williams (2006) present two views of Gaussian processes. In general, a Gaussian process (GP) defines a probability distribution over functions. Gaussian process regression allows inference in this function space (the ``function-space'' view). The weight-space view of GPs extends the general Bayesian linear model to a high-dimensional feature space. In this feature space it is sometimes possible to use the ``kernel trick'' to simplify computations in high dimensional settings.

Starting with the basic Bayesian linear model with Gaussian noise
\begin{eqnarray*}
 y        &=&    f(x) + \epsilon \\
 f(x)     &=&    x^{\prime}w \\
 \epsilon &\sim& N(0, \sigma^2),
\end{eqnarray*} 
the weight space view of Gaussian process regression allows the inputs $x$ to be projected into feature space. This offers an advantage when, for example, features for a classification problem may not be linearly separable in their original space, but could be when the projection is applied. To accomplish this, we introduce a function $\phi(x)$ to project the $D$-dimensional input into a $N$ dimensional space. This mapping $\phi(x)$ replaces $x$ in the above model so that 
\begin{eqnarray*}
 f(x)  &=& \phi(x)^\prime w.
\end{eqnarray*}
The predictive distribution $f_*$ becomes
\begin{eqnarray*}
f_* | x_*, X, y &\sim& N(\phi(x_*)^\prime \Sigma_p \Phi(K+\sigma^2_n I)^{-1}y, \\
&& \phi(x_*)^\prime \Sigma_p \phi_* - \phi(x_*)^\prime \Sigma_p \Phi(K + \sigma^2 I)^{-1} \Phi^\prime \Sigma_p \phi(x_*)).
\end{eqnarray*}
In cases where an algorithm is defined solely by inner products in input space, we can simplify the computation of $f_*$ by defining $k(x_1, x_2)=\phi(x_1)^\prime \Sigma_p \phi(x_2)=\Sigma^{1/2}_p \phi(x_1) \cdot \Sigma^{1/2}_p \phi(x_2)$. Then, we can replace occurrences of inner products by $k(x_1, x_2$ (the ``kernel trick'').

Gaussian process regression can also be understood according to the function space view, as a collection of random variables with a joint Gaussian distribution. However, we may not know the full covariance matrix of the distribution (if we did, we would have a full Gaussian distribution rather than a GP). In this case, we define a covariance function using the same kernel as above:
\begin{eqnarray*}
\text{cov}(f(x_1), f(x_2)) &=& k(x_1, x_2) = \exp(-\frac{1}{2} |x_1 - x_2|^2).
\end{eqnarray*}
This covariance function defines a distribution over functions (hence, ``function space''). It allows us to sample a subset of input points $X_*$, fill in the covariance matrix for these points, and generate a Gaussian vector
\begin{eqnarray*}
f_* &\sim^ N(0, K(X_*, X_*)).
\end{eqnarray*}
We can extend this to the using of training and test sets, as above. The outcome of GP inference, then, is a posterior over the function space and a covariance between candidate functions. The GP is a powerful tool that can also be extended to regression with multiple target variables and a number of other applications.



\end{document}
