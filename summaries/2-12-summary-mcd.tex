\documentclass[12pt,letterpaper]{article}
\include{preamble}

% Edit these as appropriate
\newcommand\course{STA571/CS590.01}
\newcommand\semester{Spring 2014}                   % <-- current semester
\newcommand\papertitle{Hierarchical Beta Processes and the Indian Buffet Process}                         % <-- paper title
\newcommand\authoryear{Thibaux and Jordan, AISTATS, 2007}
\newcommand\yourname{Matt Dickenson}                % <-- your name
\newcommand\login{mcd31}                            % <-- your NetID
\newcommand\hwdate{Due: 12 February, 2014}           % <-- HW due date


\pagestyle{fancyplain}
\headheight 60pt
\chead{Summary of ``\papertitle''\\ ~\\}
\lhead{\small \yourname\ \texttt{\login}\\\course}
\rhead{\small \hwdate}
\headsep 10pt

\begin{document}

% \noindent \emph{Homework Notes:} 

% Read and Summarize "Hierarchical Beta Processes and the Indian Buffet Process" by Thibaux and Jordan, AISTATS, 2007.

Thibaux and Jordan (2007) build on the development of the Indian Buffet Process (IBP) as a prior for binary matrices as introduced by Griffiths and Ghahramani (2005). Griffiths and Ghahramani show that a reordering of feature matrices, as generated by the IBP, into their ``left-ordered form'' (i.e. treating the columns as binary numbers and sorting in descending order) produces a class of equivalent matrices. Thus, the IBP is an exchangeable distribution over binary matrices. By de Finetti's theorem, any infinitely exchangeable sequence can be written as $P(Z_1, \ldots, Z_n) = \int \{ \prod_{i=1}^n P(Z_i | B) \} d P(B)$. This means that there exists a random element $B$ that renders the $Z_i$ conditionally independent. 

% How are the IBP and Beta process related?

Thibaux and Jordan show that the beta process serves as $P(B)$ for matrices generated by the IBP. That is, the Beta process is the de Finetti mixing distribution underlying the Indian buffet process, just as the Dirichlet process is for the CRP. This parallel aids in the development of the two-parameter generalization of the IBP, in which $\pi_k | \alpha, \beta \sim \text{Beta}(\frac{\alpha \beta}{K}, \beta)$. As in the one-parameter IBP, the first customer fills her plate with a Poisson($\alpha$) number of dishes. Then, the $i^{th}$ customer tastes previously sampled dishes, with probability $\frac{m_k}{\beta + i - 1}$, where $m_k$ is the number of previous customers who sampled dish $k$. The $i^{th}$ customer then samples an additional Poisson($\alpha \beta /(\beta+i-1)$) new dishes. The average number of dishes per customer (i.e. features per object) is still $\alpha$, but the expected number of overall features and non-zero columns is $\alpha \sum_{i=1}^n \frac{\beta}{\beta+i-1}$. $\beta$ gives us control over the size and sparsity of the matrix, decoupling the average number of features from the total. Understanding the beta process as the basis for the IBP also shows how a hierarchical version can be developed, allowing sets of features to be shared across nested data. Thibaux and Jordan's contribution helps to deepen our understanding of the IBP and extend it to new applications.






\end{document}
